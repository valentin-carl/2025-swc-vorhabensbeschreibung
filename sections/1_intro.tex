\clearpage
\section{Task Definition and Motivation}

% Formulation of the scientific question: Social relevance, practical relevance, practical application example

Function-as-a-Service (FaaS) is a serverless computing model in which developers write small, stateless functions that are invoked by a cloud platform in response to external requests.
In this model, the platform manages nearly all aspects of execution, including resource allocation, auto-scaling, and the runtime environment.
Especially in edge environments, where resources are scarce, FaaS has proven to be a suitable paradigm for sharing hardware between applications and allocating resources only when they are actually needed.
At the same time, the rapid growth of cloud platforms and data centers has made their energy consumption a critical concern.
Over the past decades, global data center electricity use has steadily increased without signs of slowing down.
In 2023, data centers in the United States alone consumed \SI{176}{TWh}, accounting for \SI{4.4}{\percent} of the country's total electricity use, with projections estimating a rise to \SIrange{6.7}{12}{\percent} by 2028~\cite{shehabi_united_2024}.
Despite significant improvements in Power Usage Effectiveness (PUE), rising demand consistently outpaces efficiency gains, and data centers already account for roughly \SI{1}{\percent} of global electricity consumption~\cite{masanet_recalibrating_2020,sharma_jevons_2017,gandhi_metrics_2023}.
This trajectory directly conflicts with the Paris Agreement's target of limiting global warming to well below \SI{2}{\celsius}, which requires rapid and substantial reductions in greenhouse gas emissions.
These developments create a responsibility for both developers and cloud platforms alike to consciously consider and continuously improve the environmental footprint of their infrastructures~\cite{chien_driving_2021}.

The energy demand of artificial intelligence workloads is a particularly pressing issue.
Modern inference tasks are heavily GPU-bound, and while GPUs provide the necessary computational performance, they are also associated with high energy costs.
This creates a fundamental tension between society's growing reliance on AI-powered services and the urgent need to reduce the carbon footprint of digital infrastructures.
To address this challenge, we focus on FaaS as the underlying paradigm.
The serverless model provides fine-grained elasticity, centralized infrastructure management, resource sharing across applications, and a large degress of control to the cloud platform, which are particularly valuable both in large-scale cloud data centers and in resource-constrained edge environments.
These properties make FaaS a natural foundation for implementing energy-aware orchestration strategies that can adaptively control GPU usage.
At the same time, a lot of work remains to be done in order to improve the currently poor energy efficiency of contemporary FaaS platforms~\cite{sharma_challenges_2023}.
The scientific question that motivates this project is therefore: How can GPU resources for serverless inference be orchestrated in an adaptive and energy-efficient manner, without compromising performance?

The social relevance of this question lies in the sustainability of digital infrastructures.
As AI models become important components of everyday applications, from medical diagnostics to large language models, operators must reconcile latency and throughput demands with climate responsibility.
Practically, today's serverless platforms provide limited support for GPU execution, in general, and fine-grained GPU management, in particular: In most deployments and when available, GPUs remain powered even during idle times, wasting significant amounts of energy.
Addressing this inefficiency not only has the potential to greatly affect environmental impact of serverless infrastructure but also lowers operational costs in cloud-scale environments.
This issue is even more pressing for Germany, as the country already experiences substatially increased levels of warming compared to global trends and currently even the most pessimistic RPC8.5 scenario~\cite{dwd_2024_klimastatusbericht}.

Practical application examples can be found in inference-as-a-service offerings.
Applications such as real-time medical image analysis or conversational AI systems must respond elastically to fluctuating demand.
A dynamic orchestration approach will ensure that GPUs are powered on only when required, while predictive scheduling mechanisms mitigate cold-start overheads.
This will enable cloud providers and companies using private clouds to deliver sustainable, latency-sensitive services at scale without sacrificing user experience.
The goal of the \textbf{GEKO} (\enquote{\textbf{G}PUs \textbf{e}nergieeffizient f√ºr \textbf{K}I-Inferenz \textbf{o}rchestrieren}) project is therefore to develop serverless platform architectures and programming abstractions that will enable deploying, managing, and using serverless functions with GPU support for AI applications in an energy-efficient manner.
\needcheck{The platform and abstractions will be published as Open-Source, so that other researchers and industry can directly benefit from the results of the project.}

\subsection{Focus and objectives}
% Grobe Ziele, Menthodenschwerpunkte, Neuheutsgrad des Ansatzes

FaaS has emerged as a promising abstraction for building scalable and elastic applications.
However, despite its advantages, current FaaS platforms remain highly energy-inefficient~\cite{sharma_challenges_2023}. 
This inefficiency stems from two key factors: the strong variance in request loads, which often leads to overprovisioning or idle resources, and the expensive software-level isolation required to execute short-lived functions securely~\cite{schirmer2023nightshift,ginzburg_serverless_2020}.
As a result, serverless applications today are far from exploiting their potential for sustainable operation.

At the same time, FaaS has the ideal prerequisites to serve as the core programming model for energy-efficient AI inference~\cite{patros_2021_towards_sustainable_serverless}.
Its fine-grained elasticity, centralized control of resources, and abstraction from application logic make it a natural fit for orchestrating energy-aware scheduling and adaptive GPU usage. 
Yet, realizing this potential requires foundational research, since existing platforms offer only limited support in this direction. 
Notably, most public FaaS services do not provide GPU support at all, leaving no basis for exploring efficient orchestration of inference workloads.

A key reason why FaaS is particularly well suited for sustainable computing lies in its platform-centric model.
Instead of requiring every developer to solve sustainability challenges individually, the serverless abstraction concentrates responsibility for efficiency at the platform level.
This enables resource sharing, workload consolidation, and energy-aware scheduling to be implemented once and leveraged by all applications running on the platform.
In principle, this makes FaaS one of the strongest candidates for aligning large-scale digital infrastructures with sustainability goals, provided that the necessary system mechanisms exist.

The focus of this project is therefore to establish the groundwork for sustainable, scalable GPU-based inference in serverless environments.
We aim to provide the missing system-level mechanisms that allow GPUs to be integrated into FaaS platforms and managed adaptively with respect to workload demands.
In doing so, the GEKO project seeks to bridge the gap between today's energy-inefficient serverless platforms and a future in which FaaS is the foundation of sustainable AI infrastructure.


\subsection{Scientific and/or technical objectives of the project}
% Ausdifferenzierung, technische Konkretisierung. Typisches Ziel: Prototyp, Demonstrator - keine Produktentwicklung, kein fertiges Produkt

Building on the motivation outlined above, this project aims to develop the foundations for sustainable and scalable GPU-based inference in serverless platforms. The overarching goal is to transform Function-as-a-Service from an energy-inefficient abstraction into a viable basis for sustainable AI infrastructure. To achieve this, we focus on system-level mechanisms that enable adaptive GPU orchestration, efficient resource sharing, and transparent integration of energy-aware scheduling policies into the serverless execution model.

The concrete objectives of this project are threefold. First, we seek to design and implement the missing platform mechanisms that allow GPUs to be exposed as first-class resources in FaaS environments. Second, we aim to develop orchestration strategies that adapt GPU allocation dynamically to workload fluctuations, minimizing idle energy costs while preserving performance. Third, we plan to evaluate the effectiveness of these strategies across a diverse set of inference workloads and deployment settings, thereby quantifying their impact on both energy efficiency and quality of service.

From these objectives, the following technical and research questions emerge:
\begin{enumerate}
    \item How can GPUs be exposed and managed as first-class resources in FaaS environments, given the short-lived and highly dynamic nature of serverless functions? % GPU Integration
    \item What orchestration strategies can dynamically adapt GPU allocation to workload fluctuations, ensuring high utilization while minimizing idle energy costs? % Adaptive Scheduling
    \item How do the proposed mechanisms perform across diverse AI inference workloads, and what trade-offs emerge between energy efficiency, performance, and scalability? % Evaluation and trade-offs
\end{enumerate}

To address them, the project will create an open-source prototype of a serverless platform that integrates GPU support and implements energy-aware orchestration mechanisms.
This prototype will be designed to be usable and extensible by both the research community and industry practitioners, providing a practical foundation for future work on sustainable AI infrastructures.


\subsection{Relation of the project to funding policy objectives/funding program}

% Zielstellung Software Campus, inhaltlicher Bezug zu Zukunftsstrategie des BMBF https://www.bmbf.de/bmbf/de/forschung/zukunftsstrategie/zukunftsstrategie_node.html

The project GEKO is closely aligned with the strategic goals of the BMFTR and the objectives of its \enquote{Hightech Agenda Deutschland}.
In particular, it addresses two central themes emphasized in the funding policy: advancing artificial intelligence as a key enabling technology and promoting sustainable digital infrastructures in line with Germany's climate commitments.

First, GEKO contributes to strengthening Germany's technological leadership in AI by addressing the high energy demand of inference workloads.
The project develops innovative system-level mechanisms for energy-efficient orchestration of GPU resources in serverless environments.
Consequently, it complements existing AI research that predominantly focuses on model efficiency and instead tackles the infrastructure and platform perspective, which are equally important for the practical use of AI technology.
This is directly in line with the Hightech Agenda's objective to expand AI research across the full technology stack and secure digital sovereignty in Europe.
Second, GEKO has a strong relation to the BMFTR's climate and sustainability goals.
The project's central objective, i.e., reducing the carbon footprint of AI workloads in cloud platforms, directly supports Germany's contribution to achieving the climate targets of the Paris Agreement.
By focusing on platform-level orchestration and resource sharing, GEKO demonstrates how sustainability can be built into digital infrastructures rather than being left to individual developers or applications.
This systemic approach has the potential to deliver significant energy savings at scale, thereby making a measurable contribution to sustainable digitalization.

Finally, the project strengthens education and innovation transfer.
Embedded in the Software Campus program, GEKO provides graduate students with the opportunity to develop practical expertise in systems research, cloud platforms, and sustainable computing.
In parallel, the project fosters leadership and soft skills that are crucial for future roles in academia and industry.
The planned open-source prototype of a serverless GPU platform ensures that the results are not only of academic value but also accessible to the wider research community and industrial stakeholders, thereby accelerating innovation transfer and supporting Germany's role as a leading hub for sustainable AI.
In this way, GEKO not only advances fundamental research but also facilitates direct knowledge transfer from academia to industry, ensuring that insights into sustainable AI infrastructures quickly translate into practical innovations in the German and European technology sector.