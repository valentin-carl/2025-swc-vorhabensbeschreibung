\clearpage
\section{State of the Art in Science and Technology}
\label{sec:standWiss}

% - Nationale / internationale Arbeiten und die Unterschiede; eigene Vorarbeiten; (Abgrenzung der eigenen Forschungsarbeit muss deutlich werden)
% - Hinweis auf wichtigste aktuelle Projekte / Methoden (keine akademisch vollständigen Beschreibungen, Beschreibung muss zielbezogen sein, Referenzen möglich, aber nicht notwendig)
% - Abgrenzung zu aktuellen und laufenden Vorhaben mit Förderkatalog der Bundesregierung (\url{https://foerderportal.bund.de/foekat/jsp/StartAction.do}) und früheren SWC Projekten

\newcommand{\absatz}[1]{\emph{#1.}}

Function-as-a-Service is a serverless programming model in which developers express applications as small, stateless functions that are invoked on demand by the cloud platform. 
This abstraction frees developers from concerns about scalability and infrastructure management, while enabling large-scale resource sharing across many tenants. 
Combined with a pay-per-use billing model, FaaS has quickly become a central paradigm in both industry and research for building elastic applications~\cite{jonas_cloud_2019}.
Importantly, this model also has untapped potential as a cornerstone for sustainable computing: If orchestrated carefully, shared resources can be provisioned more efficiently at platform level than if each developer had to optimize for sustainability individually.
The GEKO project builds on a solid foundation of scientific groundwork laid by a small but active research community in the field of sustainable serverless computing. 
This includes early explorations into how serverless abstractions could become enablers for energy-aware resource management and carbon accounting. 
A cornerstone in this emerging area is the articulation of a broader vision for sustainable serverless computing, which frames FaaS as a potential driver of greener cloud services and highlights key research gaps that remain open~\cite{sharma_challenges_2023}.

In today's practice, however, serverless computing is far from energy efficient.
Current FaaS deployments follow two dominant paths. 
On the one hand, organizations operate open-source serverless platforms on top of Kubernetes or similar orchestration systems. 
While this approach offers flexibility, it suffers from heavy reliance on virtualization and container isolation, which introduces substantial overheads; these, in turn, increase per-request energy consumption between 15$\times$ and 30$\times$, depending on the virtualization technique used~\cite{sharma_challenges_2023}.
Recent studies show that software-based isolation layers such as gVisor can significantly degrade efficiency, limiting the potential for sustainable operation~\cite{young_true_nodate}. 
On the other hand, large public cloud providers offer commercial FaaS services, which remain largely opaque to researchers and offer only limited resource control. %(\needcheck{EXAMPLES}).
In particular, GPU support is mostly absent from these platforms, making it difficult to realize AI workloads in a serverless fashion.
Despite these limitations, the research community has begun to address sustainability in serverless computing. 
Early work has explored quantifying the energy overheads of function isolation and assigning carbon intensity metrics to individual function invocations~\cite{sharma_accountable_2024}.
Other studies have examined spatio-temporal scheduling, where functions are steered to data centers with lower grid carbon intensity. %(\needcheck{EXAMPLES, viele}). 
However, these approaches often conflict with the latency requirements of serverless workloads and cannot fully exploit hardware-level optimizations~\cite{sukprasert_limitations_2024}. 
The majority of existing work still focuses on performance-oriented goals, such as reducing cold start latencies, rather than systematically reducing the energy footprint of the platform. %(\needcheck{EXAMPLES, viele}).
In the context of artificial intelligence, the gap is even more pronounced. 
Modern inference workloads are increasingly GPU-bound, but the lack of GPU integration in public serverless platforms is a contributor to preventing FaaS from being used for scalable AI inference. 
While research in AI sustainability has made progress on model-level and hardware-level optimizations, the platform dimension, i.e., deciding how, when, and which hardware is activated for inference, remains largely unexplored. % hier gab's noch nen guten Huawei Kommentar zu aber will ich erstmal so lassen
As a result, serverless computing today does not yet realize its potential as a key abstraction for sustainable AI inference workloads.

Against this background, the GEKO project addresses a clear research gap.
Unlike public FaaS platforms, GEKO builds on an open-source foundation that allows transparent investigation of GPU integration and orchestration. 
In contrast to existing open-source solutions, it focuses not only on enabling GPU support but also on making the serverless platform itself responsible for hardware usage decisions. 
This system-level focus avoids the mismatch between application-level assumptions and platform-level realities, ensuring that resources are shared and utilized as efficiently as possible. 
As a result, GEKO directly advances the state of the art in sustainable FaaS and establishes the groundwork for scalable, energy-efficient AI inference.
